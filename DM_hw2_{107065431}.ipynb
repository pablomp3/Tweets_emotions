{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   ------------------------------------------------------------------------------------------------------------\n",
    "##   ----------------------------------------  DATA MINING  ---------------------------------------------\n",
    "\n",
    "##   ----------------------------------------  Homework #2  ---------------------------------------------\n",
    "##   ------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Ëç∑Ë•ø Pablo L√≥pez Di√©guez \n",
    "\n",
    "**Student ID:** 107065431\n",
    "\n",
    "**Team name:** Vidaloka\n",
    "\n",
    "\n",
    "The snapshot of the Private Leaderboard is attached:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/que1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen from the **Private Leaderboard**, the result obtained was **0.47435 (80-Benchmark)**. The different approaches utilised for this Homework #2 will be studied and discussed in this report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a nice dataframe from input data\n",
    "This part is basically for importing the data we will work with, create nice train/test dataframes and save them into pickle files, which have several advantages, such as those regarding space requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x2a8830</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x20b21d</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0x2452cf</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0x2d729d</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x2ab56d</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id identification\n",
       "0  0x28cc61           test\n",
       "1  0x29e452          train\n",
       "2  0x2b3819          train\n",
       "3  0x2db41f           test\n",
       "4  0x2a2acc          train\n",
       "5  0x2a8830          train\n",
       "6  0x20b21d          train\n",
       "7  0x2452cf          train\n",
       "8  0x2d729d          train\n",
       "9  0x2ab56d          train"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# Read and import emotion.csv\n",
    "emotion = pd.read_csv('emotion.csv')\n",
    "#print(emotion[0:10])\n",
    "# Read and import data_id.csv\n",
    "data_id = pd.read_csv('data_identification.csv')\n",
    "#data_id.shape\n",
    "data_id[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and import the actual file with info\n",
    "dm = pd.read_json('tweets_DM.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>texts</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>0x218443</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[GodsPlan, GodsWork]</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[]</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>0x26289a</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[]</td>\n",
       "      <td>Turns out you can recognise people by their un...</td>\n",
       "      <td>0x31c6e0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[sheltered]</td>\n",
       "      <td>I like how Hayvens mommy, daddy, and the keybo...</td>\n",
       "      <td>0x32edee</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[notamused]</td>\n",
       "      <td>I just love it when every single one of my son...</td>\n",
       "      <td>0x3714ee</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[CelebrityBigBrother]</td>\n",
       "      <td>@JulieChen when can we expect a season of #Cel...</td>\n",
       "      <td>0x235628</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>[]</td>\n",
       "      <td>Tbh. Regret hurts more than stepping on a LEGO...</td>\n",
       "      <td>0x283024</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             hashtags  \\\n",
       "2                        [bibleverse]   \n",
       "4                                  []   \n",
       "9   [materialism, money, possessions]   \n",
       "30               [GodsPlan, GodsWork]   \n",
       "33                                 []   \n",
       "35                                 []   \n",
       "37                        [sheltered]   \n",
       "46                        [notamused]   \n",
       "49              [CelebrityBigBrother]   \n",
       "56                                 []   \n",
       "\n",
       "                                                texts  tweet_id identification  \n",
       "2   Confident of your obedience, I write to you, k...  0x28b412           test  \n",
       "4   \"Trust is not the same as faith. A friend is s...  0x2de201           test  \n",
       "9   When do you have enough ? When are you satisfi...  0x218443           test  \n",
       "30  God woke you up, now chase the day #GodsPlan #...  0x2939d5           test  \n",
       "33  In these tough times, who do YOU turn to as yo...  0x26289a           test  \n",
       "35  Turns out you can recognise people by their un...  0x31c6e0           test  \n",
       "37  I like how Hayvens mommy, daddy, and the keybo...  0x32edee           test  \n",
       "46  I just love it when every single one of my son...  0x3714ee           test  \n",
       "49  @JulieChen when can we expect a season of #Cel...  0x235628           test  \n",
       "56  Tbh. Regret hurts more than stepping on a LEGO...  0x283024           test  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to isolate the column _source of dm file, since the other columns are not interesting\n",
    "# tweet_source is a dictionary that we want to transform later into a dataframe\n",
    "tweet_source = dm['_source']\n",
    "tweet_source[0:10]\n",
    "#type(tweet_source)\n",
    "#tweet_source.to_frame()\n",
    "\n",
    "\n",
    "# create three arrays --> columns in new dataframe\n",
    "hashtags = []\n",
    "texts = []\n",
    "tweet_id = []\n",
    "\n",
    "# store info of the dictionary into a new dataframe\n",
    "for row in tweet_source:\n",
    "   hashtags += [row['tweet'][ 'hashtags']]\n",
    "   texts += [row['tweet']['text']]\n",
    "   tweet_id += [row['tweet']['tweet_id']]\n",
    "\n",
    "# create the new dataframe with 3 columns\n",
    "htt = {'hashtags': hashtags, 'texts': texts, 'tweet_id': tweet_id}\n",
    "df = pd.DataFrame(data=htt)\n",
    "df[0:10]\n",
    "#df.shape\n",
    "\n",
    "# Merge the new dataframe and the identification dataset, based on tweets with the same id\n",
    "bigdf = pd.merge(df, data_id, on=\"tweet_id\")\n",
    "bigdf[0:50]\n",
    "#bigdf.shape\n",
    "\n",
    "data_id_train_without_emotion = bigdf[bigdf['identification'] == 'train']\n",
    "data_id_train_without_emotion[0:10]\n",
    "#data_id_train.shape\n",
    "\n",
    "# Length for emotion dataframe is the same as the length for training dataframe, so we can\n",
    "# divide the complete dataframe (bigdf) in both train and test, and then merge the trainning \n",
    "# dataframe with the emotions\n",
    "\n",
    "# Add emotion info to the train dataframe based on tweet id\n",
    "df_train = pd.merge(data_id_train_without_emotion, emotion, on=\"tweet_id\")\n",
    "df_train[0:10]\n",
    "# shape is lower than before\n",
    "#df_train.shape\n",
    "\n",
    "df_test = bigdf[bigdf['identification'] == 'test']\n",
    "df_test[0:10]\n",
    "#df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pickle format:** We choose to save the dataframes that have just been created into pickle format. These dataframes will be imported into **Kaggle** for easy and quick access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train and test dataframes into pickle format\n",
    "df_train.to_pickle(\"train_df.pkl\")\n",
    "df_test.to_pickle(\"test_df.pkl\")\n",
    "\n",
    "## load a pickle file\n",
    "train_df = pd.read_pickle(\"train_df.pkl\")\n",
    "test_df = pd.read_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following part was the code run on Kaggle, which consists in some cleaning data and pre-processing engineering, building a model, training it and finally obtaining an output file that allow us to check the accuracy of our work.\n",
    "\n",
    "\n",
    "//------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "It is noted that, since the original code was run on **Kaggle**, some pieces of the code cannot be re-run here, since nor **TensorFlow** nor **Keras** are available in my local computer. Therefore, screenshots of the most significant results will be displayed when corresponds.\n",
    "\n",
    "//------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Firstly, **numpy** and **pandas** were imported, although for this report it is not strictly neccesary to include them, since it was done before.\n",
    "\n",
    "We also show the beggining of the train dataframe that was created before importing into **Kaggle**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code cannot be run in this **Jupyter** Notebook, therefore a screenshot will show the performance on **Kaggle**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/im1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>texts</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "2                             []   \n",
       "3      [authentic, LaughOutLoud]   \n",
       "4                             []   \n",
       "\n",
       "                                               texts  tweet_id identification  \\\n",
       "0  People who post \"add me on #Snapchat\" must be ...  0x376b20          train   \n",
       "1  @brianklaas As we see, Trump is dangerous to #...  0x2d5350          train   \n",
       "2                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>  0x1cd5b0          train   \n",
       "3  @RISKshow @TheKevinAllison Thx for the BEST TI...  0x1d755c          train   \n",
       "4       Still waiting on those supplies Liscus. <LH>  0x2c91a8          train   \n",
       "\n",
       "        emotion  \n",
       "0  anticipation  \n",
       "1       sadness  \n",
       "2          fear  \n",
       "3           joy  \n",
       "4  anticipation  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    People who post \"add me on #Snapchat\" must be ...\n",
       "1    @brianklaas As we see, Trump is dangerous to #...\n",
       "2                  Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>\n",
       "3    @RISKshow @TheKevinAllison Thx for the BEST TI...\n",
       "4         Still waiting on those supplies Liscus. <LH>\n",
       "Name: texts, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['texts'][0:5] #train_df.texts[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several approaches for removing components of tweets were used. I decided to remove **: ;** etc, and different elements that will make the model to work worse. Unfortunately, removing things like the **< LH >** or mentions  (**@**) made the model to have worse accuracy, so I tried to balance everything in order to get a more accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['texts'].replace(regex=True,inplace=True,to_replace=r'\"',value=r'')\n",
    "train_df['texts'].replace(regex=True,inplace=True,to_replace=r',',value=r'')\n",
    "train_df['texts'].replace(regex=True,inplace=True,to_replace=r':',value=r'')\n",
    "train_df['texts'].replace(regex=True,inplace=True,to_replace=r';',value=r'')\n",
    "train_df['texts'].replace(regex=True,inplace=True,to_replace=r'(?u)',value=r'')\n",
    "#train_df['texts'].replace(regex=True,inplace=True,to_replace=r')',value=r'')\n",
    "#train_df['texts'].replace(regex=True,inplace=True,to_replace=r'...',value=r'')\n",
    "#train_df['texts'].replace(regex=True,inplace=True,to_replace=r'<LH>',value=r'')\n",
    "#train_df['texts'].replace(regex=True,inplace=True,to_replace=r'@',value=r'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, I made all the tweets **lowercase**.\n",
    "The hashtags remained the same, since I did not find a better performance with **hashtag** manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>texts</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>people who post add me on #snapchat must be de...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas as we see trump is dangerous to #f...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>now issa is stalking tasha üòÇüòÇüòÇ &lt;lh&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@riskshow @thekevinallison thx for the best ti...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>still waiting on those supplies liscus. &lt;lh&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hashtags  \\\n",
       "0                     [Snapchat]   \n",
       "1  [freepress, TrumpLegacy, CNN]   \n",
       "2                             []   \n",
       "3      [authentic, LaughOutLoud]   \n",
       "4                             []   \n",
       "\n",
       "                                               texts  tweet_id identification  \\\n",
       "0  people who post add me on #snapchat must be de...  0x376b20          train   \n",
       "1  @brianklaas as we see trump is dangerous to #f...  0x2d5350          train   \n",
       "2                now issa is stalking tasha üòÇüòÇüòÇ <lh>  0x1cd5b0          train   \n",
       "3  @riskshow @thekevinallison thx for the best ti...  0x1d755c          train   \n",
       "4       still waiting on those supplies liscus. <lh>  0x2c91a8          train   \n",
       "\n",
       "        emotion  \n",
       "0  anticipation  \n",
       "1       sadness  \n",
       "2          fear  \n",
       "3           joy  \n",
       "4  anticipation  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['texts'] = train_df.texts.str.lower()\n",
    "#train_df['hashtags'] = train_df.hashtags.lower()\n",
    "train_df[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe that performing a **permutation** of the tweets will increase accuracy. My tests confirm it, even though the improvement is very small, so I decided to keep it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>texts</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>564414</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>people who post add me on #snapchat must be de...</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181475</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas as we see trump is dangerous to #f...</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869327</th>\n",
       "      <td>[]</td>\n",
       "      <td>now issa is stalking tasha üòÇüòÇüòÇ &lt;lh&gt;</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246420</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>@riskshow @thekevinallison thx for the best ti...</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702622</th>\n",
       "      <td>[]</td>\n",
       "      <td>still waiting on those supplies liscus. &lt;lh&gt;</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              hashtags  \\\n",
       "564414                      [Snapchat]   \n",
       "181475   [freepress, TrumpLegacy, CNN]   \n",
       "869327                              []   \n",
       "1246420      [authentic, LaughOutLoud]   \n",
       "702622                              []   \n",
       "\n",
       "                                                     texts  tweet_id  \\\n",
       "564414   people who post add me on #snapchat must be de...  0x376b20   \n",
       "181475   @brianklaas as we see trump is dangerous to #f...  0x2d5350   \n",
       "869327                 now issa is stalking tasha üòÇüòÇüòÇ <lh>  0x1cd5b0   \n",
       "1246420  @riskshow @thekevinallison thx for the best ti...  0x1d755c   \n",
       "702622        still waiting on those supplies liscus. <lh>  0x2c91a8   \n",
       "\n",
       "        identification       emotion  \n",
       "564414           train  anticipation  \n",
       "181475           train       sadness  \n",
       "869327           train          fear  \n",
       "1246420          train           joy  \n",
       "702622           train  anticipation  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random sorting of the tweets which may increase efficiency of the prediction - feature engineering\n",
    "train_df = train_df.reindex(np.random.permutation(train_df.index))\n",
    "train_df[0:5]\n",
    "#len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'dinkster', 'wanted', 'by', 'the', 'fbi', '!', '|', '<lh>', 'üò®']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.hashtags.sort_values\n",
    "train_df['texts'][2].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the tweets were **splitted** into words and emojis. This will allow the model to analyse component by component when needed, like during the training part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "564414     [people, who, post, add, me, on, #snapchat, mu...\n",
       "181475     [@brianklaas, as, we, see, trump, is, dangerou...\n",
       "869327           [now, issa, is, stalking, tasha, üòÇüòÇüòÇ, <lh>]\n",
       "1246420    [@riskshow, @thekevinallison, thx, for, the, b...\n",
       "702622     [still, waiting, on, those, supplies, liscus.,...\n",
       "969577                  [love, knows, no, gender., üò¢üò≠, <lh>]\n",
       "1101101    [@dstvngcare, @dstvng, more, highlights, are, ...\n",
       "1262185    [the, #ssm, debate, <lh>, (a, manufactured, fa...\n",
       "336749     [i, love, suffering, üôÉüôÉ, i, love, when, valium...\n",
       "266063     [can, someone, tell, my, why, my, feeds, scrol...\n",
       "93077      [you, know, you, research, butterflies, when, ...\n",
       "1209650    [my, brother, didn't, tell, me, he, was, going...\n",
       "32423      [on, a, scale, of, kylie, jenner-heidi, klum, ...\n",
       "657848     [progress, at, house, meyer, pre, galv, sub, f...\n",
       "109797     [vomi, post, birthday, celebrations!, #late_po...\n",
       "731070     [one, of, my, dreams, come, true, a, couple, o...\n",
       "608980     [thankful, for, another, day, of, life, üôèüèæ., #...\n",
       "281527     [giving, thanks, to, my, lord, and, savior, fo...\n",
       "500597     [@grayconnolly, fmd...you, idiots, keep, spend...\n",
       "748586     [@realdonaldtrump, do, you, even, care, how, m...\n",
       "240014     [i‚Äôll, never, be, half, the, man, pat, tillman...\n",
       "837384     [when, you, just, want, to, be, with, your, fa...\n",
       "989182     [@carolinemutoko, @ukenyatta, ballot, is, stro...\n",
       "1263844    [the, fire, within, is, all, driven, by, needi...\n",
       "173898     [no, wait, okay, so, fuck, people, who, think,...\n",
       "598402     [millions, of, purple, cushion-footed, balderd...\n",
       "173082     [@usps, @uspshelp, ..., why, is, it, that, all...\n",
       "1176481    [@secy_state_us, @johnpodesta, are, u, drinkin...\n",
       "1048575    [@nufc, @skybetchamp, @nu_foundation, some, <l...\n",
       "357321     [#sundaybreakfast, @bbccornwall, , @jameslangs...\n",
       "                                 ...                        \n",
       "283852        [been, <lh>, by, patrick, n, andy, so, far, üëå]\n",
       "1296308    [i, was, literally, just, about, to, tweet, do...\n",
       "233825     [this, season, lets, show, <lh>, to, one, anot...\n",
       "657174     [hey, @cr41vjt, i, hope, you, can, <lh>, from,...\n",
       "1322228    [blessed, are, those, who, see, beautiful, thi...\n",
       "191444     [@brandonklinger, i, agree, they, were, very, ...\n",
       "933707     [@bravotv, why, can't, shannon, stfu, about, h...\n",
       "982815     [@railminindia, what, is, the, use, of, having...\n",
       "70981      [texas, shooter, identified, as, a, white, mal...\n",
       "783799     [i've, got, agust, d, stuck, in, my, head., wh...\n",
       "1089279    [i, lost, my, name, tag, for, work, ‚òπÔ∏è#noname,...\n",
       "28604      [sitting, in, a, crowd, completely, #alone., <...\n",
       "1297943    [when, was, last, time, you, were, #engaged, <...\n",
       "1242423    [#navansha, <lh>, #ultimate, <lh>, üë®üíûüëß, ‚Äî, lis...\n",
       "1192865        [bussin, our, another, 12, today, üò©ü§ëüôåüèæ, <lh>]\n",
       "842357     [i, feel, you're, so, close, to, me\\ri, can, h...\n",
       "957208     [@jackmeoffer1776, @lenadunham, you, are, a, c...\n",
       "836804     [my, <lh>, is, patient, &&, <lh>, and, shit......\n",
       "122752     [it's, amazing, how, much, you, can, learn, an...\n",
       "189746     [everyone, is, so, pleasently, unlikable, af, ...\n",
       "989146     [@sharonklaus1, , once, an, enemy, is, conquer...\n",
       "440119     [@chelseapoe666, the, love, project., let‚Äôs, d...\n",
       "1023545    [@amberrudd, and, the, rest, of, #tory, #cpc20...\n",
       "376019     [..., now, he's, doing, live, shows, on, stage...\n",
       "471720     [waking, up, with, only, a, slight, headache, ...\n",
       "794190     [i'm, so, happy!!!, #nowonder, the, name, of, ...\n",
       "746608     [in, every, circumtance, i'd, like, to, be, th...\n",
       "890246     [there's, currently, two, girls, walking, arou...\n",
       "680522     [ah, corporate, life, where, you, can, date, <...\n",
       "820515         [blessed, to, be, living, #sundayvibes, <lh>]\n",
       "Name: texts, Length: 1455563, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split all the tweets\n",
    "train_df['texts'].str.split(' ')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So now it's possible to check its type\n",
    "type(train_df['texts'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following piece of code, we use **Bag of Words/CountVectorizer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1455563x809331 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 18799895 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to use something to transform our data before we can introduce into the model. For that we would use Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# build analyzers (bag-of-words)\n",
    "BOW_vectorizer = CountVectorizer()\n",
    "# 1. Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "BOW_vectorizer.fit(train_df['texts'])\n",
    "#--------------------------------------------------------- A√ëADIDO PARA EL TEST DE CLASE -----------------------------\n",
    "#BOW_words = CountVectorizer(max_features=500, analyzer='word', stop_words='English',tokenizer=nltk.word_tokenize, ngram_range=(2,4), max_df=0.8)\n",
    "#------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 2. Transform documents to document-term matrix.\n",
    "train_data_BOW_features = BOW_vectorizer.transform(train_df['texts'])\n",
    "test_data_BOW_features = BOW_vectorizer.transform(test_df['texts'])\n",
    "\n",
    "# check the result\n",
    "train_data_BOW_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part consists on the usage of **Keras** and **Tensorflow**, which was done in **Kaggle** as well.\n",
    "**Bag of Words** was utilized with **20.000 features** instead of the 500 used in the lab, so the name of BOW_500 was not changed for reference. The more features, the better the performance of the model. However, there was a big issue regarding this topic, which was the running time of the kernel in Kaggle, that could not exceed 6h. Therefore, the number of features had to be carefully chosen in order to fulfill all the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_data_BOW_features)\n",
    "# add .toarray() to show\n",
    "train_data_BOW_features\n",
    "# check the dimension\n",
    "train_data_BOW_features.shape\n",
    "# (there are 10115 words)\n",
    "# observe some feature names\n",
    "feature_names = BOW_vectorizer.get_feature_names()\n",
    "feature_names[100000:100010]\n",
    "\"üòÇ\" in feature_names\n",
    "# we can check that the emoji is not included in our data\n",
    "import nltk\n",
    "\n",
    "# build analyzers (bag-of-words)\n",
    "BOW_500 = CountVectorizer(max_features=20000, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "# apply analyzer to training data\n",
    "BOW_500.fit(train_df['texts'])\n",
    "\n",
    "train_data_BOW_features_500 = BOW_500.transform(train_df['texts'])\n",
    "\n",
    "## check dimension\n",
    "train_data_BOW_features_500.shape\n",
    "train_data_BOW_features_500.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check how the identification of **emojis** is going on now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe some feature names\n",
    "feature_names_500 = BOW_500.get_feature_names()\n",
    "feature_names_500[100:110]\n",
    "\"üòÇ\" in feature_names_500\n",
    "# now the emoji is included in our data (we used the second 'advanced scissor' - by adjusting tokenizer and feature numbers)\n",
    "# now we have the numerical vector that we can put in our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided not to include more feature engineering techniques with the other emojis from the train dataset, since after several tries, the accuracy of the model wouldn't increase even by setting stop parameters to certain emojis. I believe this is due to lots of emojis being used with an ironic meaning:\n",
    "\n",
    "For example, I may use a **happy face** together with a text that presents a **negative emotion**, since the internet slang does not need to correspond with the social features of real life.\n",
    "\n",
    "//---------------------------------------------------------------\n",
    "\n",
    "\n",
    "Import **Keras** and **Tensorflow** on **Kaggle**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check keras & tensorflow work\n",
    "import keras\n",
    "# standardize name (X, y) \n",
    "X_train = BOW_500.transform(train_df['texts'])\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "X_test = BOW_500.transform(test_df['texts'])\n",
    "#y_test = test_df['emotion']\n",
    "\n",
    "## check dimension is a good habbit \n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "#print('y_test.shape: ', y_test.shape)\n",
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:8]:\\n', y_train[0:8])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "#print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "#y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:8]:\\n', y_train[0:8])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "#print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/im2_1.png)\n",
    "![title](img/im3_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the output obtained in **Kaggle Notebook** corresponding to the latter piece of code. I decided to show the most relevant pieces of output code by means of screenshots directly from Kaggle Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model.\n",
    "#### At the beginning a Decision Tree and Naive Bayes was explored, but discarded after low performance and accuracy was achieved. Therefore, it was decided to create a model based on the one used in the lab session number 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built model\n",
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **input shape** was 2000 while the **output shape** corresponded to 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=45)(X)  # 45\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=45)(H1)  # 45\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  #\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/im4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After several tries, the **chosen model**, which is specified in detail in the previous screenshot, included:\n",
    "1. Input layer\n",
    "2. First hidden layer\n",
    "3. Second hidden layer\n",
    "4. Output layer\n",
    "\n",
    "The **shape** of the hidden layers was reduced to 45 for improving accuracy, by trial-and-error method. I needed to test it several times to find a good result.\n",
    "\n",
    "I believe that, for my model, the number of **3 epochs** reaches a higher accuracy, because increasing epochs results in an **overtraining** of the model. The batch size has worked better being decreased in a certain degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training setting\n",
    "epochs = 3 # podemos cambiar el tiempo de entreno/prueba y error\n",
    "batch_size = 15\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>texts</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>0x28b412</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>0x2de201</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>0x218443</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[GodsPlan, GodsWork]</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[]</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>0x26289a</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             hashtags  \\\n",
       "2                        [bibleverse]   \n",
       "4                                  []   \n",
       "9   [materialism, money, possessions]   \n",
       "30               [GodsPlan, GodsWork]   \n",
       "33                                 []   \n",
       "\n",
       "                                                texts  tweet_id identification  \n",
       "2   Confident of your obedience, I write to you, k...  0x28b412           test  \n",
       "4   \"Trust is not the same as faith. A friend is s...  0x2de201           test  \n",
       "9   When do you have enough ? When are you satisfi...  0x218443           test  \n",
       "30  God woke you up, now chase the day #GodsPlan #...  0x2939d5           test  \n",
       "33  In these tough times, who do YOU turn to as yo...  0x26289a           test  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P R E D I C T I O N\n",
    "\n",
    "Finally, after the model was trained, it is time to **predict!!!** It's so exciting having reached this step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict\n",
    "pred_result = model.predict(X_test, batch_size=100)\n",
    "pred_result[:5]\n",
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "pred_result[:5]\n",
    "type(pred_result)\n",
    "pred_result[0:5]\n",
    "\n",
    "from pandas import DataFrame\n",
    "columns = ['emotion']\n",
    "tuit_id = test_df['tweet_id']\n",
    "                  \n",
    "prediction = DataFrame(pred_result, columns = columns)\n",
    "#prediction\n",
    "\n",
    "prediction_final = {'id' : tuit_id, 'emotion' : pred_result}\n",
    "prediction_final = pd.DataFrame(prediction_final)\n",
    "prediction_final[0:10]\n",
    "#len(prediction_final)\n",
    "\n",
    "prediction_final.to_csv('./final_prediction1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "prediction_final[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After this prediction step, a CSV file with two columns (tweet id and its corresponding prediction) was generated on Kaggle, returning the accuracy obtained when submited to the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "I believe that, if I had more time, I could have tried other models more complex, that would allow me to eventually reach a higher score. Unfortunately, time was not enough to keep on exploring further options. I have talked to other classmates and the key could be in a good **cleaning** of the data and **feature engineering**, so I think I could have explored this option more, besided of course, trying new **CNN models**.\n",
    "\n",
    "Selecting a good **trainning parameters** was also important, since it is easy to over-train the model as a newbee applying data mining techniques, by simply thinking that a more deep training will clearly produce a better performance, which seems to be not always right.\n",
    "\n",
    "I also think that the 6h of **Kaggle** may be a limitation, although I am not sure about the quantity of features usually selected in other data mining projects that obtain much higher accuracy levels (around 70%).\n",
    "\n",
    "Overall, I am satisfied with my model in general and with the accuracy obtained, and I hope this report can help me to get a good score ;)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
